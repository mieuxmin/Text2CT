# Multi-Gene Neuroimaging Alignment

111ê°œì˜ gene embeddingì„ neuroimaging latentì™€ aligní•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ë²•

### 1. **Single Gene Mode** - ê°œë³„ Gene Alignment
ê° geneì„ ê°œë³„ì ìœ¼ë¡œ neuroimagingê³¼ aligní•©ë‹ˆë‹¤.

```bash
python scripts/alignment/train_multi_gene_alignment.py \
    --mode single \
    --single_gene_name APOE \
    --output_dir ./outputs/single_gene/APOE
```

**ì¥ì **:
- ê° geneì˜ ë…ë¦½ì ì¸ ì˜í–¥ íŒŒì•…
- ê°„ë‹¨í•œ ëª¨ë¸ êµ¬ì¡°
- ë¹ ë¥¸ í•™ìŠµ

**ì‚¬ìš© ì‚¬ë¡€**:
- íŠ¹ì • geneì˜ neuroimaging ì—°ê´€ì„± ë¶„ì„
- Geneë³„ ë…ë¦½ì ì¸ ì˜ˆì¸¡ ëª¨ë¸

### 2. **Sequence Mode** - Multi-Gene Transformer Alignment
111ê°œì˜ geneì„ sequenceë¡œ ì²˜ë¦¬í•˜ì—¬ transformer encoderë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

```bash
python scripts/alignment/train_multi_gene_alignment.py \
    --mode sequence \
    --output_dir ./outputs/multi_gene_sequence
```

**ì¥ì **:
- Gene ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ
- ì „ì²´ì ì¸ ìœ ì „ì í”„ë¡œíŒŒì¼ í™œìš©
- CLIPì˜ text encoderì™€ ìœ ì‚¬í•œ êµ¬ì¡°

**ì‚¬ìš© ì‚¬ë¡€**:
- ë³µí•©ì ì¸ ìœ ì „ì-ë‡Œ ê´€ê³„ ëª¨ë¸ë§
- ë‹¤ì¤‘ geneì„ í™œìš©í•œ neuroimaging ì˜ˆì¸¡

## ğŸ“ ë°ì´í„° êµ¬ì¡°

```
/scratch/connectome/mieuxmin/Brain_Gene_FM/
â”œâ”€â”€ APOE_brain_gene_embeddingUKB.csv      # IID + 256 embedding dims
â”œâ”€â”€ BDNF_brain_gene_embeddingUKB.csv
â”œâ”€â”€ ...
â””â”€â”€ {gene_name}_brain_gene_embeddingUKB.csv  # ì´ 111ê°œ

/storage/bigdata/UKB_LDM/autoencoder_output/run_77481/brain_latent/
â”œâ”€â”€ 1234567_latent.npz   # shape: (3, 15, 18, 15)
â”œâ”€â”€ 1234568_latent.npz
â””â”€â”€ ...
```

## ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°

### Single Gene Mode

```
Neuro (12150) â”€â”€â†’ [Linear] â”€â”€â†’ L2-Normalize â”€â”€â”
                                                â”‚
                                                â”œâ”€â”€â†’ Contrastive Loss
                                                â”‚
Gene (256)    â”€â”€â†’ [Linear] â”€â”€â†’ L2-Normalize â”€â”€â”˜
```

### Sequence Mode (Transformer)

```
Neuro (12150) â”€â”€â†’ [Linear] â”€â”€â†’ L2-Normalize â”€â”€â”€â”€â”€â”€â”
                                                    â”‚
                                                    â”œâ”€â”€â†’ Contrastive Loss
                                                    â”‚
Gene Sequence                                       â”‚
(111, 256)                                          â”‚
    â”‚                                               â”‚
    â”œâ”€â”€â†’ [Input Projection] â”€â”€â†’ (111, 512)         â”‚
    â”‚                                               â”‚
    â”œâ”€â”€â†’ [Positional Encoding]                     â”‚
    â”‚                                               â”‚
    â”œâ”€â”€â†’ [Transformer Encoder]                     â”‚
    â”‚    - 4 layers                                 â”‚
    â”‚    - 8 attention heads                        â”‚
    â”‚    - GELU activation                          â”‚
    â”‚                                               â”‚
    â”œâ”€â”€â†’ [Pooling: mean/max/cls] â”€â”€â†’ (512)         â”‚
    â”‚                                               â”‚
    â””â”€â”€â†’ [Projection] â”€â”€â†’ L2-Normalize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ì˜µì…˜ 1: Single Gene Training (ê°œë³„ gene)

```bash
# APOE geneë§Œ í•™ìŠµ
python scripts/alignment/train_multi_gene_alignment.py \
    --mode single \
    --single_gene_name APOE \
    --batch_size 128 \
    --num_epochs 50 \
    --projection_dim 512 \
    --output_dir ./outputs/single_gene/APOE

# ëª¨ë“  geneì— ëŒ€í•´ ë°˜ë³µ ì‹¤í–‰
for gene in APOE BDNF COMT ...; do
    python scripts/alignment/train_multi_gene_alignment.py \
        --mode single \
        --single_gene_name $gene \
        --output_dir ./outputs/single_gene/$gene
done
```

### ì˜µì…˜ 2: Multi-Gene Sequence Training

```bash
# ê¸°ë³¸ ì„¤ì • (mean pooling)
python scripts/alignment/train_multi_gene_alignment.py \
    --mode sequence \
    --batch_size 64 \
    --num_epochs 100 \
    --projection_dim 512 \
    --transformer_hidden_dim 512 \
    --transformer_num_layers 4 \
    --transformer_num_heads 8 \
    --transformer_pooling mean \
    --output_dir ./outputs/multi_gene_sequence

# CLS token pooling (BERT-style)
python scripts/alignment/train_multi_gene_alignment.py \
    --mode sequence \
    --transformer_pooling cls \
    --output_dir ./outputs/multi_gene_sequence_cls

# ë” í° transformer
python scripts/alignment/train_multi_gene_alignment.py \
    --mode sequence \
    --transformer_hidden_dim 768 \
    --transformer_num_layers 6 \
    --transformer_num_heads 12 \
    --batch_size 32 \
    --gradient_accumulation_steps 2 \
    --output_dir ./outputs/multi_gene_large
```

## ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°€ì´ë“œ

### Single Gene Mode

| Parameter | Recommended | Description |
|-----------|------------|-------------|
| `--batch_size` | 128 | ë” í´ìˆ˜ë¡ ì¢‹ìŒ |
| `--projection_dim` | 512 | ê³µìœ  ê³µê°„ ì°¨ì› |
| `--learning_rate` | 1e-4 | í•™ìŠµë¥  |
| `--num_epochs` | 50-100 | ë¹ ë¥´ê²Œ ìˆ˜ë ´ |

### Sequence Mode

| Parameter | Recommended | Description |
|-----------|------------|-------------|
| `--batch_size` | 32-64 | TransformerëŠ” ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš© |
| `--projection_dim` | 512 | ê³µìœ  ê³µê°„ ì°¨ì› |
| `--transformer_hidden_dim` | 512-768 | Transformer ë‚´ë¶€ ì°¨ì› |
| `--transformer_num_layers` | 4-6 | Layer ê°œìˆ˜ |
| `--transformer_num_heads` | 8-12 | Attention head ê°œìˆ˜ |
| `--transformer_pooling` | mean/cls | Pooling ë°©ë²• |
| `--learning_rate` | 1e-4 | í•™ìŠµë¥  |
| `--num_epochs` | 100-200 | TransformerëŠ” ë” ì˜¤ë˜ í•™ìŠµ |

## ğŸ”¬ Transformer Pooling ë°©ë²•

### 1. Mean Pooling (ê¸°ë³¸ê°’)
- ëª¨ë“  gene embeddingì˜ í‰ê· 
- ê°€ì¥ ì•ˆì •ì 
- ëª¨ë“  geneì„ ë™ë“±í•˜ê²Œ ê³ ë ¤

```python
output = transformer_output.mean(dim=1)  # (B, hidden_dim)
```

### 2. Max Pooling
- ê° ì°¨ì›ì˜ ìµœëŒ€ê°’
- ì¤‘ìš”í•œ feature ê°•ì¡°

```python
output = transformer_output.max(dim=1)[0]  # (B, hidden_dim)
```

### 3. CLS Token (BERT-style)
- í•™ìŠµ ê°€ëŠ¥í•œ special token ì¶”ê°€
- Sequence ì „ì²´ ì •ë³´ë¥¼ ì••ì¶•

```python
cls_token = learnable_parameter  # (1, 1, hidden_dim)
x = concat([cls_token, gene_sequence], dim=1)
output = transformer(x)[:, 0, :]  # Use CLS token
```

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ

ì‹¤í—˜ ì„¤ì •ì— ë”°ë¥¸ ì˜ˆìƒ ì„±ëŠ¥:

| Mode | Model Size | Training Time | Memory | Accuracy* |
|------|-----------|---------------|--------|-----------|
| Single | ~6M params | 10 min/epoch | 2GB | 0.65-0.75 |
| Sequence (small) | ~15M params | 30 min/epoch | 8GB | 0.70-0.80 |
| Sequence (large) | ~30M params | 60 min/epoch | 16GB | 0.75-0.85 |

*Accuracy: Cross-modal retrieval top-1 accuracy

## ğŸ’¡ ì‚¬ìš© ì˜ˆì œ

### í•™ìŠµ í›„ Inference

```python
import torch
from scripts.alignment.multi_gene_dataset import MultiGeneNeuroDataset
from scripts.alignment.multi_gene_model import MultiGeneAlignmentModel

# Load model
model = MultiGeneAlignmentModel(
    neuro_input_dim=12150,
    gene_input_dim=256,
    num_genes=111,  # sequence mode
    projection_dim=512,
    use_transformer=True,
    transformer_hidden_dim=512,
    transformer_num_layers=4,
    transformer_num_heads=8,
)

checkpoint = torch.load('outputs/multi_gene_sequence/best_model.pt')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Load data
dataset = MultiGeneNeuroDataset(
    brain_latent_dir='/storage/bigdata/UKB_LDM/autoencoder_output/run_77481/brain_latent',
    gene_embedding_dir='/scratch/connectome/mieuxmin/Brain_Gene_FM',
    mode='sequence',
)

# Get a sample
sample = dataset[0]
neuro_emb = sample['neuro_embedding'].unsqueeze(0)  # (1, 12150)
gene_seq = sample['gene_sequence'].unsqueeze(0)     # (1, 111, 256)

# Encode
with torch.no_grad():
    neuro_feat = model.encode_neuro(neuro_emb)      # (1, 512)
    gene_feat = model.encode_gene(gene_seq)         # (1, 512)

    # Compute similarity
    similarity = (neuro_feat * gene_feat).sum()
    print(f"Similarity: {similarity.item():.4f}")
```

### Gene ê°„ Attention ì‹œê°í™”

```python
# Extract attention weights from transformer
# (sequence modeì—ì„œë§Œ ê°€ëŠ¥)

import matplotlib.pyplot as plt
import seaborn as sns

# Forward pass with attention weights
model.gene_encoder.transformer.layers[0].self_attn.register_forward_hook(
    lambda module, input, output: attention_weights.append(output[1])
)

attention_weights = []
_ = model(neuro_emb, gene_seq)

# Visualize attention (111 x 111)
attn = attention_weights[0][0].mean(0).cpu().numpy()  # Average over heads

plt.figure(figsize=(12, 10))
sns.heatmap(attn, xticklabels=dataset.gene_names, yticklabels=dataset.gene_names)
plt.title('Gene-Gene Attention Weights')
plt.tight_layout()
plt.savefig('gene_attention.png')
```

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### 1. Gene íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ

```
ValueError: No gene embedding files found
```

**í•´ê²°ì±…**:
- `--gene_embedding_dir` ê²½ë¡œ í™•ì¸
- `*_brain_gene_embeddingUKB.csv` íŒŒì¼ í˜•ì‹ í™•ì¸

### 2. Memory ë¶€ì¡± (Sequence Mode)

```
CUDA out of memory
```

**í•´ê²°ì±…**:
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
--batch_size 16

# Gradient accumulation ì‚¬ìš©
--gradient_accumulation_steps 4

# Transformer í¬ê¸° ì¤„ì´ê¸°
--transformer_hidden_dim 256
--transformer_num_layers 2
```

### 3. íŠ¹ì • Geneì´ ì—†ìŒ (Single Mode)

```
ValueError: Gene 'XXX' not found
```

**í•´ê²°ì±…**:
```python
# ì‚¬ìš© ê°€ëŠ¥í•œ gene ëª©ë¡ í™•ì¸
from scripts.alignment.multi_gene_dataset import MultiGeneNeuroDataset

genes = MultiGeneNeuroDataset.get_all_gene_names(
    '/scratch/connectome/mieuxmin/Brain_Gene_FM'
)
print(f"Available genes: {genes}")
```

## ğŸ“š ì°¸ê³  ìë£Œ

- **CLIP**: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- **Transformer**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- **BERT**: [Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)

## ğŸ“ ë…¼ë¬¸ ì‘ì„± ì‹œ ì¸ìš©

```bibtex
@article{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and others},
  journal={International Conference on Machine Learning},
  year={2021}
}
```
